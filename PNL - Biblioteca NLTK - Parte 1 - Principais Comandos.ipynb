{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.5)\n"
     ]
    }
   ],
   "source": [
    "# Processamento de Linguagem Natural com Python e NLTK\n",
    "# Autor: José Heenrike - Extração de Conhecimento e Mineração de Dados.\n",
    "\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo do Texto para Validação.\n",
    "# Texto =  'Mr. Green is portrayed as a plump, pompous looking business man with grey hair. On the box, he is wearing a grayish suit with a green tie, while on his card, his suit is brown.'\n",
    "Texto =  'O diretor de um hospital da Faixa de Gaza afirmou que mais de 50 palestinos foram mortos em um ataque aéreo contra um campo de refugiados em Jabalia, norte do enclave. Na mesma região, o Exército de Israel também disse ter matado um dos principais líderes do Hamas em Gaza, confirmando o ataque aéreo.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O diretor de um hospital da Faixa de Gaza afirmou que mais de 50 palestinos foram mortos em um ataque aéreo contra um campo de refugiados em Jabalia, norte do enclave', ' Na mesma região, o Exército de Israel também disse ter matado um dos principais líderes do Hamas em Gaza, confirmando o ataque aéreo', '']\n"
     ]
    }
   ],
   "source": [
    "#Texto = 'estou contente com o resultado do teste que fiz no dia de ontem - Comando Slit para separa as palavras'\n",
    "print(Texto.split('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O diretor de um hospital da Faixa de Gaza afirmou que mais de 50 palestinos foram mortos em um ataque aéreo contra um campo de refugiados em Jabalia, norte do enclave.', 'Na mesma região, o Exército de Israel também disse ter matado um dos principais líderes do Hamas em Gaza, confirmando o ataque aéreo.']\n"
     ]
    }
   ],
   "source": [
    "# O tokenize tem como função..Separar os blocos de parágrafos de acordo com a semântica\n",
    "Frases = nltk.tokenize.sent_tokenize(Texto)\n",
    "print(Frases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'diretor', 'de', 'um', 'hospital', 'da', 'Faixa', 'de', 'Gaza', 'afirmou', 'que', 'mais', 'de', '50', 'palestinos', 'foram', 'mortos', 'em', 'um', 'ataque', 'aéreo', 'contra', 'um', 'campo', 'de', 'refugiados', 'em', 'Jabalia', ',', 'norte', 'do', 'enclave', '.', 'Na', 'mesma', 'região', ',', 'o', 'Exército', 'de', 'Israel', 'também', 'disse', 'ter', 'matado', 'um', 'dos', 'principais', 'líderes', 'do', 'Hamas', 'em', 'Gaza', ',', 'confirmando', 'o', 'ataque', 'aéreo', '.']\n"
     ]
    }
   ],
   "source": [
    "# O comando word_tokenize faz a quebra dos blocos em palavras e separa cada uma delas de acordo com seu tipo.\n",
    "tokens = nltk.word_tokenize(Texto)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('O', 'NNP'), ('diretor', 'NN'), ('de', 'IN'), ('um', 'JJ'), ('hospital', 'NN'), ('da', 'NN'), ('Faixa', 'NNP'), ('de', 'IN'), ('Gaza', 'NNP'), ('afirmou', 'NN'), ('que', 'NN'), ('mais', 'NN'), ('de', 'IN'), ('50', 'CD'), ('palestinos', 'NNS'), ('foram', 'JJ'), ('mortos', 'NNS'), ('em', 'VBP'), ('um', 'JJ'), ('ataque', 'NN'), ('aéreo', 'NN'), ('contra', 'NN'), ('um', 'JJ'), ('campo', 'NN'), ('de', 'IN'), ('refugiados', 'FW'), ('em', 'FW'), ('Jabalia', 'NNP'), (',', ','), ('norte', 'RB'), ('do', 'VBP'), ('enclave', 'VB'), ('.', '.'), ('Na', 'NNP'), ('mesma', 'NN'), ('região', 'NN'), (',', ','), ('o', 'JJ'), ('Exército', 'NNP'), ('de', 'IN'), ('Israel', 'NNP'), ('também', 'NNP'), ('disse', 'NN'), ('ter', 'NN'), ('matado', 'NN'), ('um', 'JJ'), ('dos', 'NN'), ('principais', 'NN'), ('líderes', 'NNS'), ('do', 'VBP'), ('Hamas', 'NNP'), ('em', 'VB'), ('Gaza', 'NNP'), (',', ','), ('confirmando', 'NN'), ('o', 'NN'), ('ataque', 'NN'), ('aéreo', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# O método pos_tag identifica cada palavra e seleciona o seu tipo de acordo com a gramática exemplo, adjetivo, verbo, substantitivo, etc.\n",
    "classes = nltk.pos_tag(tokens)\n",
    "print(classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  O/NNP\n",
      "  diretor/NN\n",
      "  de/IN\n",
      "  um/JJ\n",
      "  hospital/NN\n",
      "  da/NN\n",
      "  (PERSON Faixa/NNP)\n",
      "  de/IN\n",
      "  (LOCATION Gaza/NNP)\n",
      "  afirmou/NN\n",
      "  que/NN\n",
      "  mais/NN\n",
      "  de/IN\n",
      "  50/CD\n",
      "  palestinos/NNS\n",
      "  foram/JJ\n",
      "  mortos/NNS\n",
      "  em/VBP\n",
      "  um/JJ\n",
      "  ataque/NN\n",
      "  aéreo/NN\n",
      "  contra/NN\n",
      "  um/JJ\n",
      "  campo/NN\n",
      "  de/IN\n",
      "  refugiados/FW\n",
      "  em/FW\n",
      "  (GPE Jabalia/NNP)\n",
      "  ,/,\n",
      "  norte/RB\n",
      "  do/VBP\n",
      "  enclave/VB\n",
      "  ./.\n",
      "  Na/NNP\n",
      "  mesma/NN\n",
      "  região/NN\n",
      "  ,/,\n",
      "  o/JJ\n",
      "  Exército/NNP\n",
      "  de/IN\n",
      "  (GPE Israel/NNP)\n",
      "  também/NNP\n",
      "  disse/NN\n",
      "  ter/NN\n",
      "  matado/NN\n",
      "  um/JJ\n",
      "  dos/NN\n",
      "  principais/NN\n",
      "  líderes/NNS\n",
      "  do/VBP\n",
      "  (PERSON Hamas/NNP)\n",
      "  em/VB\n",
      "  (PERSON Gaza/NNP)\n",
      "  ,/,\n",
      "  confirmando/NN\n",
      "  o/NN\n",
      "  ataque/NN\n",
      "  aéreo/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# O método chunk.ne_chunk tem como base identificar as palavras classificando-as como Entidades.\n",
    "entidades = nltk.chunk.ne_chunk(classes)\n",
    "print(entidades)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
